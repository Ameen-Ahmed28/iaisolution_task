import streamlit as st
from pathlib import Path
import sys

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent))

from multi_modal_rag_system import MultiModalRAGSystem


# Page config
st.set_page_config(
    page_title="Multi-Modal RAG | Document Intelligence",
    page_icon="ğŸ“„",
    layout="wide",
    initial_sidebar_state="expanded",
)

# Custom CSS
st.markdown("""
    <style>
    .main { max-width: 1200px; }
    .stTabs [data-baseweb="tab-list"] button { font-size: 18px; }
    .metric-card { background: #f0f2f6; padding: 20px; border-radius: 10px; }
    </style>
""", unsafe_allow_html=True)


@st.cache_resource
def init_rag_system():
    """Initialize RAG system once and cache it"""
    try:
        rag = MultiModalRAGSystem(use_groq=True)
        st.success("âœ… RAG System initialized")
        return rag
    except Exception as e:
        st.error(f"âŒ Failed to initialize: {str(e)}")
        st.stop()


def main():
    # Header
    st.markdown("# ğŸ“„ Multi-Modal Document Intelligence")
    st.markdown(
        "**AI-Powered QA System** | Extract insights from PDF, images, and tables using Groq + unstructured.io"
    )

    # Initialize RAG
    rag = init_rag_system()

    # Sidebar
    with st.sidebar:
        st.header("âš™ï¸ Settings")

        # Provider selection
        use_groq = st.radio(
            "Select LLM Provider:",
            [True, False],
            format_func=lambda x: "ğŸš€ Groq (Faster)" if x else "ğŸ”„ OpenRouter (Diverse)",
        )

        # Retrieval settings
        k_docs = st.slider("Documents to retrieve:", 2, 10, 4)
        temperature = st.slider("Response creativity:", 0.0, 1.0, 0.3)

        # About
        st.divider()
        st.markdown(
            """
        ### ğŸ¯ Features
        - ğŸ“‘ Multi-modal ingestion (PDF, images)
        - ğŸ” Semantic search with RAG
        - ğŸ§  Free LLM inference (Groq/OpenRouter)
        - ğŸ“ Citation tracking
        
        ### ğŸ“š Free Models
        **Groq:**
        - Mixtral 8x7B
        - Llama 2 70B
        
        **OpenRouter:**
        - Llama 2 7B
        - Mistral 7B
        """
        )

    # Main content - Tabs
    tab1, tab2, tab3, tab4 = st.tabs(
        ["ğŸ’¬ Chat", "ğŸ“¤ Upload Document", "ğŸ“Š Documents", "â„¹ï¸ About"]
    )

    # ============= TAB 1: Chat =============
    with tab1:
        st.header("ğŸ’¬ Ask Questions About Your Documents")

        # Initialize session state
        if "messages" not in st.session_state:
            st.session_state.messages = []

        # Display chat history
        for message in st.session_state.messages:
            with st.chat_message(message["role"]):
                st.markdown(message["content"])
                if "sources" in message:
                    st.caption(f"ğŸ“š Sources: {', '.join(message['sources'])}")

        # Input
        query = st.chat_input("Ask a question about your documents...")

        if query:
            # User message
            st.session_state.messages.append({"role": "user", "content": query})
            with st.chat_message("user"):
                st.markdown(query)

            # Generate answer
            with st.chat_message("assistant"):
                with st.spinner("ğŸ¤” Thinking..."):
                    try:
                        result = rag.qa_pipeline(query, k=k_docs, use_groq=use_groq)

                        # Answer
                        st.markdown(result["answer"])

                        # Sources
                        if result["sources"]:
                            st.caption(
                                f"ğŸ“š **Sources:** {', '.join(set(result['sources']))}"
                            )

                        # Store in session
                        st.session_state.messages.append(
                            {
                                "role": "assistant",
                                "content": result["answer"],
                                "sources": result["sources"],
                            }
                        )

                    except Exception as e:
                        st.error(f"âŒ Error: {str(e)}")
                        st.info("ğŸ’¡ Make sure documents are added first (see 'Upload Document' tab)")

    # ============= TAB 2: Upload Document =============
    with tab2:
        st.header("ğŸ“¤ Upload & Index Documents")

        uploaded_file = st.file_uploader(
            "Choose a file (PDF or Image)",
            type=["pdf", "png", "jpg", "jpeg"],
            help="Supports PDF files and images. Will extract text, tables, and perform OCR.",
        )

        if uploaded_file:
            # Save temp file
            temp_path = Path("./temp_upload")
            temp_path.mkdir(exist_ok=True)
            file_path = temp_path / uploaded_file.name

            with open(file_path, "wb") as f:
                f.write(uploaded_file.getbuffer())

            # Process document
            with st.spinner("ğŸ“¥ Processing document..."):
                try:
                    # Ingest
                    docs = rag.ingest_document(str(file_path))
                    st.success(f"âœ… Extracted {len(docs)} elements")

                    # Chunk
                    chunked = rag.chunk_documents(docs, chunk_size=512, chunk_overlap=100)
                    st.info(f"ğŸ“Š Chunked into {len(chunked)} segments")

                    # Index
                    rag.add_documents_to_index(chunked)
                    st.success("âœ… Documents indexed and ready for Q&A")

                    # Show extracted elements
                    with st.expander("ğŸ‘ï¸ Preview Extracted Elements"):
                        for i, doc in enumerate(docs[:5]):  # Show first 5
                            st.subheader(f"Element {i+1}: {doc.metadata.get('element_type')}")
                            st.text(doc.page_content[:300] + "...")

                except Exception as e:
                    st.error(f"âŒ Processing error: {str(e)}")

    # ============= TAB 3: Documents =============
    with tab3:
        st.header("ğŸ“Š Indexed Documents")

        if rag.docstore:
            # Statistics
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("Total Documents", len(rag.docstore))
            with col2:
                st.metric("Vector DB Size", len(rag.vectorstore.get()["documents"]))
            with col3:
                st.metric("Avg. Doc Length", 
                         int(sum(len(d.page_content) for d in rag.docstore.values()) / len(rag.docstore)))

            # Document list
            st.subheader("ğŸ“‘ Document List")
            for i, (doc_id, doc) in enumerate(list(rag.docstore.items())[:10]):
                with st.expander(
                    f"ğŸ“„ {doc.metadata.get('source', 'Unknown')} - "
                    f"{doc.metadata.get('element_type', 'Text')} "
                    f"({len(doc.page_content)} chars)"
                ):
                    st.text(doc.page_content[:500])

            # Show more button
            if len(rag.docstore) > 10:
                st.info(f"ğŸ“š Showing 10 of {len(rag.docstore)} documents")

        else:
            st.info("ğŸ“­ No documents indexed yet. Upload one to get started!")

    # ============= TAB 4: About =============
    with tab4:
        st.header("â„¹ï¸ About This System")

        st.markdown(
            """
        ## ğŸ—ï¸ Architecture
        
        This Multi-Modal RAG system combines:
        
        1. **Document Processing** - `unstructured.io`
           - Extracts text, tables, images from PDF
           - Performs OCR on scanned documents
           - Preserves document structure
        
        2. **Embeddings** - `HuggingFace (BAAI/bge-small-en-v1.5)`
           - Local embeddings (no API needed)
           - Fast and efficient
        
        3. **Vector Database** - `Chroma`
           - Semantic search and retrieval
           - Persistent local storage
        
        4. **LLM Inference** - `Groq` / `OpenRouter`
           - Free tier models available
           - Ultra-fast response times
        
        ## ğŸš€ Key Features
        
        - âœ… Multi-modal document ingestion
        - âœ… Automatic table extraction
        - âœ… OCR for scanned images
        - âœ… Semantic search with RAG
        - âœ… Citation tracking
        - âœ… Production-ready error handling
        - âœ… Fully free (no API costs for embeddings)
        
        ## ğŸ“Š Performance
        
        - **Document Processing**: 2-5 sec per document
        - **Query Latency**: 1-3 sec per question
        - **Embedding Model**: BAAI/bge-small (~100M params)
        - **Inference**: Groq (2-5 tokens/sec), OpenRouter (1-3 tokens/sec)
        
        ## ğŸ”§ Technology Stack
        
        - **Framework**: LangChain
        - **Document Parsing**: unstructured.io
        - **Vector DB**: Chroma
        - **Embeddings**: HuggingFace
        - **UI**: Streamlit
        - **LLM APIs**: Groq + OpenRouter (FREE)
        
        ## ğŸ“ Citation Format
        
        Answers include source citations like: `[Source: filename.pdf]`
        
        This ensures full traceability and accountability of retrieved information.
        """
        )

        st.divider()

        # API Status
        st.subheader("ğŸ”Œ API Status")
        col1, col2 = st.columns(2)

        with col1:
            import os

            if os.getenv("GROQ_API_KEY"):
                st.success("âœ… Groq API configured")
            else:
                st.warning("âš ï¸ Groq API not configured")

        with col2:
            if os.getenv("OPENROUTER_API_KEY"):
                st.success("âœ… OpenRouter API configured")
            else:
                st.warning("âš ï¸ OpenRouter API not configured")


if __name__ == "__main__":
    main()